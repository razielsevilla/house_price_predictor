PHASE 1: Imports
-> load the tools you’ll use for everything that follows. 
-> define the capabilities of your script/notebook.

Import Requirements:
    a. Data Handling Imports   
        -> laading data sets
        -> handling missing values
        -> creating or transforming columns
        -> converting list or arrays to numerical data
    b. Visualization Imports   
        -> plotting distributions
        -> checking correlations
        -> comparing class balance
    c. Model Imports   
        -> training a machine learning model
        -> tuning hyperparameters
        -> measuring accuracy or precision/recall
    d. Experimenting and Improving

----------------------------------

PHASE 2: DATA LOADING
-> bring your raw dataset into memory so you can inspect and process it

What to Do?
    a. Choose appropriate loader: pd.read_csv, pd.read_excel, sklearn.datasets, or database connectors.
    b. Read using options that help (e.g., parse_dates, dtype, usecols) to prevent surprises.
    c. Immediately check shape and head.

Why this matters
    a. Correct reading avoids type errors (dates read as strings, large integers losing precision).
    b. Early checks catch file encoding/format problems.

Pitfalls
    a. CSV with different delimiter (e.g., ;) — use sep=';'.
    b. Large files: consider chunksize or reading only needed columns.

Checks
    c. df.info(), df.head(), df.sample(5), df.dtypes
    d. File encoding errors: UnicodeDecodeError — try encoding='utf-8' or latin1.
    
----------------------------------

PHASE 3: DATA CLEANING AND EXPLORATION
-> understand data distribution, detect missing values/outliers, discover relationships and target balance.

What to do
    a. Summary statistics: df.describe() (numeric) and df.describe(include=['O']) (categorical).
    b. Missing values: df.isnull().sum() and visualize sns.heatmap(df.isnull(), cbar=False).
    c. Univariate plots: histograms, boxplots for numeric features.
    d. Categorical distributions: bar plots, counts.
    e. Correlations: df.corr() and heatmap for numeric features.
    f. Investigate relationships between features and target.

Why this matters
    a. EDA informs preprocessing choices (how to handle missing values, which features to transform).
    b. Detects label imbalance that will affect model choice and evaluation.

Pitfalls
    a. Ignoring outliers that skew models (sometimes you should transform instead of removing).
    b. Doing EDA on the entire dataset after you already looked at test data — avoid leaking test information.

Checks
    c. Confirm target balance; if heavily imbalanced, plan strategies (resampling, class weights).

----------------------------------

PHASE 4: FEATURE ENGINEERING
-> convert raw data into features a model can learn from.

What to Do?
    a. Missing values: impute with mean/median/mode or model-based imputation.
    b. Encoding categorical variables: OneHotEncoder, OrdinalEncoder, or target encoding.
    c. Feature scaling: StandardScaler or MinMaxScaler for distance-based models (KNN, SVM).
    d. Create new features (feature engineering): interaction terms, datetime features (year/month/day), counts, ratios.
    e. Drop irrelevant columns (IDs, free-text you’re not using).

Why this matters
    a. Pipelines prevent data leakage (preprocessing steps are fit only on training data inside cross-validation).
    b. Clear preprocessing keeps code reproducible and portable.

Pitfalls
    a. Encoding categories before splitting (do it inside a pipeline).
    b. Scaling target variable (only scale features, not labels — unless regression where you transform back later).

Checks
    a. After fit_transform on train, check pipeline.named_steps['preprocessor'].transformers_ to ensure expected columns transformed.
    b. Confirm shapes and no NaNs propagate into model.

----------------------------------

PHASE 5: DATA SPLITTING  
-> separate data so you can train models and evaluate generalization

Typical Splitting
    a. Train / Test: e.g., train_test_split(X, y, test_size=0.2, random_state=42)
    b. For hyperparameter tuning: use cross-validation (GridSearchCV) or separate validation set.

Why this matters
    a. Proper splitting avoids optimistic performance estimates.
    b. stratify=y keeps class proportions consistent across splits (important for imbalanced classes).

Pitfalls
    a. Doing preprocessing (like scaling or imputing) using whole dataset before splitting → data leakage.
    b. Using test set during feature selection/tuning.

Checks
    a. Confirm shapes: X_train.shape, X_test.shape.
    b. Check class balance across splits with y_train.value_counts(normalize=True).

----------------------------------

PHASE 6: MODEL TRAINING AND EVALUATION
-> choose algorithms, train them, and measure how well they perform.

What to Do?
    a. Start simple (baseline models: Logistic Regression for classification, Linear Regression for regression).
    b. Use cross-validation (e.g., cross_val_score) to estimate generalization performance.
    c. Train the model on training data and evaluate on validation/test sets.
    d. Compute and compare key metrics (Accuracy, Precision, Recall, F1-Score, R², MSE depending on task).
    e. Record the results for each model and iteration.

Why this matters
    a. Baseline models provide a reference point—complex models must outperform them.
    b. Prevents overfitting by assessing model performance on unseen data.
    c. Establishes measurable metrics for model improvement and comparison.

Pitfalls
    a. Using only Accuracy for imbalanced datasets—consider F1-score, AUC, or Recall.
    b. Training and testing on the same data (leads to overly optimistic performance).
    c. Ignoring overfitting/underfitting signs (e.g., very high training accuracy but low test accuracy).

Checks
    a. Compare training vs testing performance; look for large gaps.
    b. Review classification_report for detailed precision/recall insights.
    c. Ensure random_state is set for reproducibility.

----------------------------------

PHASE 7: HYPERPARAMETER TUNING AND IMPROVEMENT
-> optimize the model’s settings to improve accuracy and generalization.

What to Do?
    a. Define parameter grids (e.g., number of neighbors, depth of trees, learning rates).
    b. Use GridSearchCV or RandomizedSearchCV for cross-validated hyperparameter optimization.
    c. Track and compare best parameters and their cross-validation scores.
    d. Re-train the final model using the best-found hyperparameters.
    e. Optionally, test advanced models or ensemble methods (Random Forest, XGBoost, etc.) for improvement.

Why this matters
    a. Default parameters rarely produce the best performance.
    b. Systematic tuning identifies configurations that maximize predictive power.
    c. Cross-validation ensures that results generalize well to unseen data.

Pitfalls
    a. Tuning on test data (must be done on training or validation data only).
    b. Using large, exhaustive grids without reason (wastes time).
    c. Overfitting to cross-validation folds instead of general improvement.

Checks
    a. Verify grid.best_params_ and grid.best_score_.
    b. Ensure improvements are consistent, not just lucky results.
    c. Store tuned models for reproducibility.

----------------------------------

PHASE 8: VISUALIZATION AND REPORTING
-> interpret model results, visualize insights, and communicate findings effectively.

What to Do?
    a. Plot performance metrics: confusion matrices, ROC curves, Precision-Recall curves.
    b. Visualize feature importances or model coefficients to explain decisions.
    c. Plot learning curves to check for underfitting/overfitting trends.
    d. Summarize metrics (Accuracy, Precision, Recall, F1, AUC) in a report or dashboard.
    e. Create visual comparisons across different models or parameter settings.

Why this matters
    a. Visualization builds interpretability and transparency—critical for trust in models.
    b. Helps identify model weaknesses (e.g., misclassified classes).
    c. Communicates results clearly to stakeholders, even non-technical ones.

Pitfalls
    a. Showing only overall accuracy without class-level insights.
    b. Ignoring visualization of feature importance or correlation—loses interpretability.
    c. Using test data repeatedly for visual tuning (data leakage).

Checks
    a. Ensure visualized data corresponds to final test predictions only.
    b. Validate metric consistency between code output and visuals.
    c. Confirm interpretability tools (e.g., SHAP, permutation importance) are applied correctly.